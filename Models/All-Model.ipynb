{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loadind Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to extract the numeric value from the string\n",
    "def extract_numeric_value(cell):\n",
    "    if isinstance(cell, str):\n",
    "        try:\n",
    "            return int(cell.split(',')[1].replace(']', '').strip())\n",
    "        except (IndexError, ValueError):\n",
    "            return np.nan  # Return NaN if parsing fails\n",
    "    return cell\n",
    "\n",
    "# Function to load and preprocess dataset\n",
    "def load_and_preprocess_data(dataset):\n",
    "    data = pd.read_csv(dataset)\n",
    "\n",
    "    # Apply the extraction function to all 'Fs' columns \n",
    "    fs_columns = [col for col in data.columns if col.startswith('Fs')]\n",
    "    for col in fs_columns:\n",
    "        data[col] = data[col].apply(extract_numeric_value)\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate the features and the target label\n",
    "    X = data.drop(columns=['Label'])\n",
    "    y = data['Label']\n",
    "\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of datasets\n",
    "datasets = [\n",
    "    'C:/Users/Natty PC/Documents/Party/Project II/PreData/Signatures/signatures-15mins.csv',\n",
    "    'C:/Users/Natty PC/Documents/Party/Project II/PreData/Signatures/signatures-30mins.csv',\n",
    "    'C:/Users/Natty PC/Documents/Party/Project II/PreData/Signatures/signatures-1hour.csv',\n",
    "    'C:/Users/Natty PC/Documents/Party/Project II/PreData/Signatures/signatures-2hours.csv',\n",
    "    'C:/Users/Natty PC/Documents/Party/Project II/PreData/Signatures/signatures-4hours.csv',\n",
    "    'C:/Users/Natty PC/Documents/Party/Project II/PreData/Signatures/signatures-8hours.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Graph Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy(results):\n",
    "    \"\"\"\n",
    "    Plots the accuracy of different models across multiple datasets.\n",
    "\n",
    "    Parameters:\n",
    "    results (list of dict): A list of dictionaries containing dataset names, model names, and their accuracies.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Loop through unique models and plot their accuracy\n",
    "    for model in df['model'].unique():\n",
    "        subset = df[df['model'] == model]\n",
    "        plt.plot(subset['dataset'], subset['accuracy'], marker='o', label=model)\n",
    "\n",
    "    # Adding titles and labels\n",
    "    plt.title('Model Accuracy across Different Datasets')\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 100)  # Set y-axis limits to 0 - 100\n",
    "    plt.legend(title='Models')\n",
    "    plt.grid()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "results=[]\n",
    "\n",
    "print(\"Decision Tree Model\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "\n",
    "    print(\"\\nX_train:\")\n",
    "    print(X_train)\n",
    "    print(\"\\ny_train:\")\n",
    "    print(y_train)\n",
    "    print(\"\\nX_test:\")\n",
    "    print(X_test)\n",
    "    print(\"\\ny_test:\")\n",
    "    print(y_test)\n",
    "    \n",
    "    # Load dataset\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data(dataset)\n",
    "\n",
    "    # Decision Tree Model\n",
    "    clf = DecisionTreeClassifier()\n",
    "\n",
    "    start_train_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    start_test_time = time.time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_test_time = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% \\nTrain Time: {end_train_time - start_train_time:.4f}s \\nTest Time: {end_test_time - start_test_time:.4f}s\")\n",
    "\n",
    "    # Store results for plotting\n",
    "    results.append({\n",
    "        'dataset': dataset.split('/')[-1],  # Get just the dataset name\n",
    "        'model': 'Decision Tree',\n",
    "        'accuracy': accuracy * 100\n",
    "    })\n",
    "\n",
    "plot_accuracy(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree - model processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results=[]\n",
    "\n",
    "# Function to visualize the decision tree rules\n",
    "def visualize_tree(clf, feature_names):\n",
    "    tree_rules = export_text(clf, feature_names=feature_names)\n",
    "    print(\"Decision Tree Rules:\\n\")\n",
    "    print(tree_rules)\n",
    "\n",
    "# Function to display a few predictions\n",
    "def show_predictions(clf, X_test, y_test):\n",
    "    print(\"Sample Predictions (First 5):\")\n",
    "    for i in range(5):  # Show first 5 examples\n",
    "        X_sample = X_test.iloc[i, :].values.reshape(1, -1)  # Get the i-th test sample\n",
    "        y_true = y_test.iloc[i]  # Actual label\n",
    "        y_pred = clf.predict(X_sample)[0]  # Predicted label\n",
    "        print(f\"Input: {X_test.iloc[i].tolist()} | Predicted: {y_pred} | Actual: {y_true}\")\n",
    "\n",
    "print(\"Decision Tree Model\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n\\nDataset: {dataset}\")\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data(dataset)\n",
    "\n",
    "    # Decision Tree Model\n",
    "    clf = DecisionTreeClassifier()\n",
    "\n",
    "    # Train the model\n",
    "    start_train_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    # Predict on test set\n",
    "    start_test_time = time.time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_test_time = time.time()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% \\nTrain Time: {end_train_time - start_train_time:.4f}s \\nTest Time: {end_test_time - start_test_time:.4f}s\")\n",
    "\n",
    "    # Visualize the decision tree\n",
    "    visualize_tree(clf, feature_names=X_train.columns.tolist())\n",
    "\n",
    "    # Show sample predictions\n",
    "    show_predictions(clf, X_test, y_test)\n",
    "\n",
    "    # Store results for plotting\n",
    "    results.append({\n",
    "        'dataset': dataset.split('/')[-1],  # Get just the dataset name\n",
    "        'model': 'Decision Tree',\n",
    "        'accuracy': accuracy * 100\n",
    "    })\n",
    "\n",
    "plot_accuracy(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "results=[]\n",
    "\n",
    "print(\"Naive Bayes Model\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data(dataset)\n",
    "\n",
    "    # Naive Bayes Model\n",
    "    nb_model = GaussianNB()\n",
    "\n",
    "    start_train_time = time.time()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    start_test_time = time.time()\n",
    "    y_pred = nb_model.predict(X_test)\n",
    "    end_test_time = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% \\nTrain Time: {end_train_time - start_train_time:.4f}s \\nTest Time: {end_test_time - start_test_time:.4f}s\")\n",
    "\n",
    "    # Store results for plotting\n",
    "    results.append({\n",
    "        'dataset': dataset.split('/')[-1],  # Get just the dataset name\n",
    "        'model': 'Naive Bayes',\n",
    "        'accuracy': accuracy * 100\n",
    "    })\n",
    "\n",
    "plot_accuracy(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niave Bayes - model processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "\n",
    "# Function to display sample predictions\n",
    "def show_predictions(nb_model, X_test, y_test):\n",
    "    print(\"\\nSample Predictions (First 5):\")\n",
    "    for i in range(5):  # Show first 5 examples\n",
    "        X_sample = X_test.iloc[i, :].values.reshape(1, -1)  # Get the i-th test sample\n",
    "        y_true = y_test.iloc[i]  # Actual label\n",
    "        y_pred = nb_model.predict(X_sample)[0]  # Predicted label\n",
    "        print(f\"Input: {X_test.iloc[i].tolist()} | Predicted: {y_pred} | Actual: {y_true}\")\n",
    "\n",
    "# Function to display feature probabilities\n",
    "def show_feature_probabilities(nb_model, feature_names):\n",
    "    print(\"\\nNaive Bayes Feature Probabilities (Mean and Variance for Each Feature Per Class):\")\n",
    "    for idx, class_label in enumerate(nb_model.classes_):\n",
    "        print(f\"\\nClass: {class_label}\")\n",
    "        print(\"Feature Means:\", nb_model.theta_[idx])  # Means of each feature per class\n",
    "        print(\"Feature Variances:\", nb_model.var_[idx])  # Variances of each feature per class\n",
    "\n",
    "print(\"Naive Bayes Model\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data(dataset)\n",
    "\n",
    "    # Naive Bayes Model\n",
    "    nb_model = GaussianNB()\n",
    "\n",
    "    # Train the model\n",
    "    start_train_time = time.time()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    # Predict on test set\n",
    "    start_test_time = time.time()\n",
    "    y_pred = nb_model.predict(X_test)\n",
    "    end_test_time = time.time()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% \\nTrain Time: {end_train_time - start_train_time:.4f}s \\nTest Time: {end_test_time - start_test_time:.4f}s\")\n",
    "\n",
    "    # Show feature probabilities for each class\n",
    "    show_feature_probabilities(nb_model, feature_names=X_train.columns.tolist())\n",
    "\n",
    "    # Show sample predictions\n",
    "    show_predictions(nb_model, X_test, y_test)\n",
    "\n",
    "    # Store results for plotting\n",
    "    results.append({\n",
    "        'dataset': dataset.split('/')[-1],  # Get just the dataset name\n",
    "        'model': 'Naive Bayes',\n",
    "        'accuracy': accuracy * 100\n",
    "    })\n",
    "\n",
    "plot_accuracy(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "results=[]\n",
    "\n",
    "print(\"SVM Model\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data(dataset)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # SVM Model\n",
    "    svm_model = SVC()\n",
    "\n",
    "    start_train_time = time.time()\n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    start_test_time = time.time()\n",
    "    y_pred = svm_model.predict(X_test_scaled)\n",
    "    end_test_time = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% \\nTrain Time: {end_train_time - start_train_time:.4f}s \\nTest Time: {end_test_time - start_test_time:.4f}s\")\n",
    "\n",
    "    # Store results for plotting\n",
    "    results.append({\n",
    "        'dataset': dataset.split('/')[-1],  # Get just the dataset name\n",
    "        'model': 'SVM',\n",
    "        'accuracy': accuracy * 100\n",
    "    })\n",
    "\n",
    "plot_accuracy(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM - model processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "results = []\n",
    "\n",
    "# Function to display sample predictions\n",
    "def show_predictions(svm_model, X_test, y_test):\n",
    "    print(\"\\nSample Predictions (First 5):\")\n",
    "    for i in range(5):  # Show first 5 examples\n",
    "        X_sample = X_test.iloc[i, :].values.reshape(1, -1)  # Get the i-th test sample\n",
    "        y_true = y_test.iloc[i]  # Actual label\n",
    "        y_pred = svm_model.predict(X_sample)[0]  # Predicted label\n",
    "        print(f\"Input: {X_test.iloc[i].tolist()} | Predicted: {y_pred} | Actual: {y_true}\")\n",
    "\n",
    "# Function to display support vectors\n",
    "def show_support_vectors(svm_model, X_train):\n",
    "    print(f\"\\nNumber of Support Vectors: {len(svm_model.support_)}\")\n",
    "    print(\"Support Vectors (First 5):\")\n",
    "    print(X_train[svm_model.support_][:5])  # Display the first 5 support vectors\n",
    "\n",
    "print(\"SVM Model\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data(dataset)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # SVM Model\n",
    "    svm_model = SVC()\n",
    "\n",
    "    # Train the model\n",
    "    start_train_time = time.time()\n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    # Test the model\n",
    "    start_test_time = time.time()\n",
    "    y_pred = svm_model.predict(X_test_scaled)\n",
    "    end_test_time = time.time()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% \\nTrain Time: {end_train_time - start_train_time:.4f}s \\nTest Time: {end_test_time - start_test_time:.4f}s\")\n",
    "\n",
    "    # Show support vectors\n",
    "    show_support_vectors(svm_model, X_train_scaled)\n",
    "\n",
    "    # Show sample predictions\n",
    "    show_predictions(svm_model, X_test, y_test)\n",
    "\n",
    "    # Store results for plotting\n",
    "    results.append({\n",
    "        'dataset': dataset.split('/')[-1],  # Get just the dataset name\n",
    "        'model': 'SVM',\n",
    "        'accuracy': accuracy * 100\n",
    "    })\n",
    "\n",
    "# Plot accuracy results\n",
    "plot_accuracy(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"CNN Model\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data(dataset)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Reshape data for CNN\n",
    "    X_train_reshaped = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)\n",
    "    X_test_reshaped = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)\n",
    "\n",
    "    # Convert labels to categorical\n",
    "    y_train_categorical = pd.get_dummies(y_train).values\n",
    "    y_test_categorical = pd.get_dummies(y_test).values\n",
    "\n",
    "    # CNN Model\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(256, activation='relu'))\n",
    "    cnn_model.add(Dropout(0.5))  # Add dropout layer for regularization\n",
    "    cnn_model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "\n",
    "    cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_train_time = time.time()\n",
    "    cnn_model.fit(X_train_reshaped, y_train_categorical, epochs=50, batch_size=64, verbose=0)\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    start_test_time = time.time()\n",
    "    y_pred_cnn = cnn_model.predict(X_test_reshaped)\n",
    "    y_pred_cnn_classes = np.argmax(y_pred_cnn, axis=1)\n",
    "    end_test_time = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_cnn_classes)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% \\nTrain Time: {end_train_time - start_train_time:.4f}s \\nTest Time: {end_test_time - start_test_time:.4f}s\")\n",
    "\n",
    "    # Store results for plotting\n",
    "    results.append({\n",
    "        'dataset': dataset.split('/')[-1],  # Get just the dataset name\n",
    "        'model': 'CNN',\n",
    "        'accuracy': accuracy * 100\n",
    "    })\n",
    "\n",
    "# Call the plotting function\n",
    "plot_accuracy(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZED CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/Natty PC/Documents/Party/Project II/PreData/Signatures/signatures-1hour.csv')\n",
    "\n",
    "# Feature extraction function\n",
    "def parse_feature(feature):\n",
    "    if pd.isna(feature):\n",
    "        return [0, 0, 0]  # Handle missing values\n",
    "    try:\n",
    "        protocol_port, value = feature.strip(\"[]\").split(\", \")\n",
    "        port = ''.join([c for c in protocol_port if c.isdigit()])\n",
    "        protocol = ''.join([c for c in protocol_port if c.isalpha()])\n",
    "        return [int(port) if port.isdigit() else 0, len(protocol), float(value)]\n",
    "    except:\n",
    "        return [0, 0, 0]  # Default if parsing fails\n",
    "\n",
    "# Apply feature parsing\n",
    "parsed_cols = []\n",
    "for col in df.columns[1:]:\n",
    "    parsed_df = df[col].apply(parse_feature).apply(pd.Series)\n",
    "    parsed_df.columns = [f'{col}_port', f'{col}_protocol_len', f'{col}_value']\n",
    "    parsed_cols.append(parsed_df)\n",
    "\n",
    "# Concatenate parsed columns with the label\n",
    "df_parsed = pd.concat([df['Label']] + parsed_cols, axis=1)\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df_parsed['Label'] = label_encoder.fit_transform(df_parsed['Label'])\n",
    "\n",
    "# One-hot encode labels\n",
    "num_classes = len(np.unique(df_parsed['Label']))\n",
    "print(f'Number of unique classes: {num_classes}')\n",
    "y = to_categorical(df_parsed['Label'], num_classes=num_classes)\n",
    "\n",
    "# Split dataset into features (X) and labels (y)\n",
    "X = df_parsed.drop(columns='Label').values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Reshape input data for CNN (samples, timesteps, features)\n",
    "# Ensure the division is possible without mismatch\n",
    "timesteps = X_train.shape[1] // 3\n",
    "if X_train.shape[1] % 3 != 0:\n",
    "    raise ValueError(\"Number of features in the dataset is not divisible by 3 for CNN input\")\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], timesteps, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], timesteps, 3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Check validation performance during training\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=128, \n",
    "                    validation_data=(X_test, y_test), class_weight=class_weights, \n",
    "                    callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_counts = df_parsed['Label'].value_counts()\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN - PLot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
