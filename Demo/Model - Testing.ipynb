{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load & Test Model ----> v.1\n",
    "# import joblib\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from memory_profiler import memory_usage\n",
    "# import time\n",
    "\n",
    "# # Load the trained model and signature list\n",
    "# model_filename = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - DCT.joblib\"\n",
    "# loaded_data = joblib.load(model_filename)\n",
    "\n",
    "# # Extract the classifier (model) and signature_list\n",
    "# clf = loaded_data['model']  # Decision tree classifier\n",
    "# signature_list = loaded_data['signature_list']  # Signature list\n",
    "\n",
    "# print(f\"Model and signature list loaded from {model_filename}\")\n",
    "\n",
    "# # Load new data (adjust file path as necessary)\n",
    "# # new_data = pd.read_csv('C:/Users/Natty PC/Documents/Party/Project II/PreData/Testing Data/data-3-1.csv')\n",
    "# new_data = pd.read_csv('C:/Users/Natty PC/Documents/Party/Project II/PreData/Testing Data/data-60-1.csv')\n",
    "\n",
    "# # Print the columns to find the true label column\n",
    "# print(\"Columns in the dataset:\")\n",
    "# print(new_data.columns)\n",
    "\n",
    "# # Preprocess new data using the same signature list\n",
    "# def extract_signature_and_frequency(cell):\n",
    "#     if isinstance(cell, str):\n",
    "#         try:\n",
    "#             signature, frequency = cell.replace('[', '').replace(']', '').split(',')\n",
    "#             return signature.strip(), int(frequency.strip())\n",
    "#         except ValueError:\n",
    "#             return None, 0  # Return None and 0 frequency on failure\n",
    "#     return None, 0\n",
    "\n",
    "# def preprocess_new_data(new_data, signature_list):\n",
    "#     fs_columns = [col for col in new_data.columns if col.startswith('Fs')]\n",
    "#     X_new = np.zeros((new_data.shape[0], len(signature_list)), dtype=int)\n",
    "\n",
    "#     for row_idx in range(new_data.shape[0]):\n",
    "#         for col in fs_columns:\n",
    "#             signature, frequency = extract_signature_and_frequency(new_data.at[row_idx, col])\n",
    "#             if signature in signature_list:\n",
    "#                 sig_idx = signature_list.index(signature)\n",
    "#                 X_new[row_idx, sig_idx] += frequency\n",
    "\n",
    "#     return X_new\n",
    "\n",
    "# # Function to test model and calculate memory usage\n",
    "# def test_model():\n",
    "#     # Start measuring time\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Preprocess new data\n",
    "#     X_new = preprocess_new_data(new_data, signature_list)\n",
    "\n",
    "#     # Make predictions with the loaded model\n",
    "#     y_pred_new = clf.predict(X_new)\n",
    "\n",
    "#     # Check for the presence of true labels\n",
    "#     true_label_column = 'Label'  # Ensure this matches exactly with the dataset\n",
    "\n",
    "#     if true_label_column not in new_data.columns:\n",
    "#         print(f\"Error: True label column '{true_label_column}' not found in the data.\")\n",
    "#         return\n",
    "\n",
    "#     # Extract true labels for accuracy calculation\n",
    "#     true_labels = new_data[true_label_column].values  # Use the updated column name\n",
    "\n",
    "#     # Map numeric predictions to class names correctly\n",
    "#     label_mapping = {\n",
    "#         0: 'Attack',\n",
    "#         1: 'Benign',          # Ensure this aligns with your true labels\n",
    "#         2: 'C&C',\n",
    "#         3: 'C&C-HeartBeat',   # Align these as per your training dataset classes\n",
    "#         4: 'DDoS',\n",
    "#         5: 'Okiru',\n",
    "#         6: 'PortScan',\n",
    "#         7: 'PortScan-Attack',\n",
    "#     }\n",
    "\n",
    "#     # Convert numeric predictions to class names\n",
    "#     y_pred_labels = [label_mapping.get(label, \"Unknown\") for label in y_pred_new]\n",
    "\n",
    "#     # Calculate accuracy\n",
    "#     accuracy = accuracy_score(true_labels, y_pred_labels)\n",
    "#     # print(f\"Accuracy of the model: {accuracy:.2f}\")\n",
    "\n",
    "#     # Print a comparison of predictions and true labels for the first 20 rows\n",
    "#     print(\"\\nPredicted \\tActual\")\n",
    "#     print(\"------------------------\")\n",
    "#     for i in range(min(20, len(y_pred_new))):  # Print only up to 20 rows\n",
    "#         print(f\"{y_pred_labels[i]}     \\t{true_labels[i]}\")\n",
    "\n",
    "#     # Show average score\n",
    "#     average_score = np.mean((true_labels == y_pred_labels).astype(float))\n",
    "#     print(f\"\\nAverage Score: {average_score:.2f}\")\n",
    "\n",
    "#     # Stop measuring time\n",
    "#     end_time = time.time()\n",
    "#     testing_time = end_time - start_time\n",
    "\n",
    "#     # Print results\n",
    "#     print(f\"\\nTesting Time: {testing_time:.10f} seconds\")\n",
    "#     print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "#     return accuracy  # Return accuracy for final calculation\n",
    "\n",
    "# # Measure memory usage during model testing\n",
    "# mem_usage = memory_usage(test_model)\n",
    "# initial_memory_usage = mem_usage[0]  # Initial memory usage\n",
    "\n",
    "# # Final memory usage\n",
    "# final_memory_usage = mem_usage[-1]  # Final memory usage\n",
    "# memory_consumption_during_testing = final_memory_usage - initial_memory_usage\n",
    "\n",
    "# # Print memory consumption results\n",
    "# print(f\"\\nInitial Memory Usage: {initial_memory_usage:.2f} KiB\")\n",
    "# print(f\"Final Memory Usage: {final_memory_usage:.2f} KiB\")\n",
    "# print(f\"Memory Consumption During Testing: {memory_consumption_during_testing:.2f} KiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# def generate_signatures_from_log(df):\n",
    "#     \"\"\"\n",
    "#     Generate signatures from the DataFrame of logs.\n",
    "    \n",
    "#     Parameters:\n",
    "#         df (DataFrame): DataFrame containing the log data.\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame: A DataFrame with signatures and their frequencies.\n",
    "#     \"\"\"\n",
    "#     # Initialize a dictionary to hold the frequency of each signature\n",
    "#     signature_counts = {}\n",
    "\n",
    "#     # Create signatures based on specified criteria\n",
    "#     for _, row in df.iterrows():\n",
    "#         # Only include 'service' if it's not '-'\n",
    "#         service = row['service'] if row['service'] != '-' else ''\n",
    "        \n",
    "#         # Create the signature string\n",
    "#         signature = f\"{row['id.resp_p']}_{row['proto']}_{service}_{row['orig_bytes']}_{row['resp_bytes']}_{row['conn_state']}\"\n",
    "        \n",
    "#         # Count frequencies\n",
    "#         if signature in signature_counts:\n",
    "#             signature_counts[signature] += 1\n",
    "#         else:\n",
    "#             signature_counts[signature] = 1\n",
    "\n",
    "#     # Prepare the output list in the desired format\n",
    "#     output_list = [[signature, frequency] for signature, frequency in signature_counts.items()]\n",
    "\n",
    "#     # Convert the output list to a DataFrame for easy processing\n",
    "#     return pd.DataFrame(output_list, columns=['signature', 'frequency'])\n",
    "\n",
    "\n",
    "# def process_log_in_intervals(file_path, model_path, interval_minutes=5):\n",
    "#     \"\"\"\n",
    "#     Read the log file in 5-minute intervals and predict using the trained model.\n",
    "    \n",
    "#     Parameters:\n",
    "#         file_path (str): Path to the log file.\n",
    "#         model_path (str): Path to the trained model.\n",
    "#         interval_minutes (int): Interval duration in minutes.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Load the log file into a DataFrame\n",
    "#         df = pd.read_csv(file_path, delim_whitespace=True, header=None,\n",
    "#                          usecols=[0, 5, 6, 7, 9, 10, 11], names=[\n",
    "#                              'ts', 'id.resp_p', 'proto', 'service', \n",
    "#                              'orig_bytes', 'resp_bytes', 'conn_state'],\n",
    "#                          na_filter=False)\n",
    "\n",
    "#         # Replace '-' or '0' in orig_bytes and resp_bytes with '0'\n",
    "#         df['orig_bytes'] = df['orig_bytes'].replace({'-': '0', '0': '0'})\n",
    "#         df['resp_bytes'] = df['resp_bytes'].replace({'-': '0', '0': '0'})\n",
    "\n",
    "#         # Convert 'ts' to datetime\n",
    "#         df['ts'] = pd.to_datetime(df['ts'], unit='s', errors='coerce')\n",
    "\n",
    "#         # Check for missing values and drop them\n",
    "#         df.dropna(subset=['id.resp_p', 'proto', 'conn_state', 'ts'], inplace=True)\n",
    "\n",
    "#         # Load the trained model\n",
    "#         model_data = joblib.load(model_path)\n",
    "#         clf = model_data['model']\n",
    "#         signature_list = model_data['signature_list']\n",
    "\n",
    "#         # Define time intervals\n",
    "#         start_time = df['ts'].min()\n",
    "#         end_time = df['ts'].max()\n",
    "#         current_time = start_time\n",
    "\n",
    "#         while current_time <= end_time:\n",
    "#             # Create time range for the current 5-minute interval\n",
    "#             next_time = current_time + pd.Timedelta(minutes=interval_minutes)\n",
    "\n",
    "#             # Filter the DataFrame for the current time range\n",
    "#             filtered_df = df[(df['ts'] >= current_time) & (df['ts'] < next_time)]\n",
    "\n",
    "#             if not filtered_df.empty:\n",
    "#                 # Generate signatures from the filtered DataFrame\n",
    "#                 signatures_df = generate_signatures_from_log(filtered_df)\n",
    "\n",
    "#                 # Prepare the signature matrix for prediction\n",
    "#                 X_matrix = np.zeros((1, len(signature_list)), dtype=int)\n",
    "                \n",
    "#                 # Populate the matrix with frequencies\n",
    "#                 for _, row in signatures_df.iterrows():\n",
    "#                     signature = row['signature']\n",
    "#                     frequency = row['frequency']\n",
    "#                     if signature in signature_list:\n",
    "#                         sig_idx = signature_list.index(signature)  # Get the index for the signature\n",
    "#                         X_matrix[0, sig_idx] += frequency  # Update the corresponding cell with the frequency\n",
    "\n",
    "#                 # Make predictions\n",
    "#                 prediction = clf.predict(X_matrix)\n",
    "\n",
    "#                 # Print results\n",
    "#                 print(f\"Time Interval: {current_time} - {next_time}\")\n",
    "#                 print(f\"Signatures: {signatures_df}\")\n",
    "#                 print(f\"Predicted Label: {prediction[0]}\")\n",
    "\n",
    "#             # Move to the next time interval\n",
    "#             current_time = next_time\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path} or {model_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "\n",
    "# # Example usage\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/IoTScenarios/CTU-IoT-Malware-Capture-3-1/bro/conn.log.txt\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - DCT.joblib\"\n",
    "# process_log_in_intervals(log_file_path, model_path, interval_minutes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def check_log_file(file_path):\n",
    "#     try:\n",
    "#         # Load the log file into a DataFrame while skipping the first 8 rows and bad lines\n",
    "#         df = pd.read_csv(file_path, delim_whitespace=True, header=None, na_filter=False, \n",
    "#                          on_bad_lines='skip', skiprows=8)\n",
    "\n",
    "#         # Display the first few rows and the number of columns\n",
    "#         print(f\"Loaded DataFrame with {df.shape[0]} records and {df.shape[1]} columns.\")\n",
    "#         print(df.head())  # Show the first few rows\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "\n",
    "# # Example usage\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/IoTScenarios/CTU-IoT-Malware-Capture-3-1/bro/conn.log.txt\"\n",
    "# check_log_file(log_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "\n",
    "# def generate_signatures_from_log(df):\n",
    "#     \"\"\"\n",
    "#     Generate signatures from the DataFrame of logs in the form [signature, frequency].\n",
    "    \n",
    "#     Parameters:\n",
    "#         df (DataFrame): DataFrame containing the log data.\n",
    "        \n",
    "#     Returns:\n",
    "#         list: A list of signatures and their frequencies.\n",
    "#     \"\"\"\n",
    "#     # Initialize a dictionary to hold the frequency of each signature\n",
    "#     signature_counts = {}\n",
    "\n",
    "#     # Create signatures based on specified criteria\n",
    "#     for _, row in df.iterrows():\n",
    "#         # Extracting necessary fields\n",
    "#         id_resp_p = row[5]  # id.resp_p\n",
    "#         proto = row[6]  # proto\n",
    "#         service = row[7]  # service\n",
    "#         orig_bytes = row[9]  # orig_bytes\n",
    "#         resp_bytes = row[10]  # resp_bytes\n",
    "#         conn_state = row[11]  # conn_state\n",
    "        \n",
    "#         # Skip service if it's '0', '-', or empty\n",
    "#         service = service if service not in ['0', '-', ''] else ''\n",
    "\n",
    "#         # Replace '-' or '0' with '0' for orig_bytes and resp_bytes\n",
    "#         orig_bytes = orig_bytes if orig_bytes not in ['-', '0'] else '0'\n",
    "#         resp_bytes = resp_bytes if resp_bytes not in ['-', '0'] else '0'\n",
    "\n",
    "#         # Create the signature string\n",
    "#         signature = f\"{id_resp_p}{proto}{service}{orig_bytes}{resp_bytes}{conn_state}\"\n",
    "        \n",
    "#         # Count frequencies\n",
    "#         if signature in signature_counts:\n",
    "#             signature_counts[signature] += 1\n",
    "#         else:\n",
    "#             signature_counts[signature] = 1\n",
    "\n",
    "#     # Prepare the output list in the desired format: [[signature, frequency], ...]\n",
    "#     output_list = [[signature, frequency] for signature, frequency in signature_counts.items()]\n",
    "\n",
    "#     return output_list  # Return a list instead of a DataFrame for the output\n",
    "\n",
    "# def process_log_in_intervals(file_path, model_path, interval_minutes=5):\n",
    "#     try:\n",
    "#         # Load the log file into a DataFrame while skipping the first 8 rows\n",
    "#         df = pd.read_csv(file_path, delim_whitespace=True, header=None, na_filter=False,\n",
    "#                          on_bad_lines='skip', skiprows=8, low_memory=False)\n",
    "\n",
    "#         # Replace non-numeric values in the timestamp column with NaN and convert\n",
    "#         df[0] = pd.to_numeric(df[0], errors='coerce')  # Convert timestamps to numeric\n",
    "        \n",
    "#         # Drop rows with NaN values in critical columns\n",
    "#         df.dropna(subset=[0, 3, 5, 6, 14, 16, 17], inplace=True)  # Ensure key columns are not NaN\n",
    "\n",
    "#         # Convert timestamp to datetime\n",
    "#         df[0] = pd.to_datetime(df[0], errors='coerce')  # Convert to datetime\n",
    "\n",
    "#         # Check for missing values after conversion\n",
    "#         df.dropna(subset=[0], inplace=True)\n",
    "\n",
    "#         # Load the trained model\n",
    "#         model_data = joblib.load(model_path)\n",
    "#         clf = model_data['model']\n",
    "#         signature_list = model_data['signature_list']\n",
    "\n",
    "#         # Define time intervals\n",
    "#         start_time = df[0].min()\n",
    "#         end_time = df[0].max()\n",
    "#         current_time = start_time\n",
    "\n",
    "#         while current_time <= end_time:\n",
    "#             # Create time range for the current interval\n",
    "#             next_time = current_time + pd.Timedelta(minutes=interval_minutes)\n",
    "\n",
    "#             # Filter the DataFrame for the current time range\n",
    "#             filtered_df = df[(df[0] >= current_time) & (df[0] < next_time)]\n",
    "\n",
    "#             if not filtered_df.empty:\n",
    "#                 # Generate signatures from the filtered DataFrame\n",
    "#                 signatures = generate_signatures_from_log(filtered_df)\n",
    "\n",
    "#                 # Prepare the signature matrix for prediction\n",
    "#                 X_matrix = np.zeros((1, len(signature_list)), dtype=int)\n",
    "\n",
    "#                 # Populate the matrix with frequencies\n",
    "#                 for sig, frequency in signatures:\n",
    "#                     if sig in signature_list:\n",
    "#                         sig_idx = signature_list.index(sig)  # Get the index for the signature\n",
    "#                         X_matrix[0, sig_idx] += frequency  # Update the corresponding cell with the frequency\n",
    "\n",
    "#                 # Make predictions\n",
    "#                 prediction = clf.predict(X_matrix)\n",
    "\n",
    "#                 # Print results\n",
    "#                 print(f\"Time Interval: {current_time} - {next_time}\")\n",
    "#                 print(f\"Signatures: {signatures}\")\n",
    "#                 print(f\"Predicted Label: {prediction[0]}\")\n",
    "\n",
    "#             # Move to the next time interval\n",
    "#             current_time = next_time\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path} or {model_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "\n",
    "# # Example usage\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/IoTScenarios/CTU-IoT-Malware-Capture-3-1/bro/conn.log.txt\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - DCT.joblib\"\n",
    "# process_log_in_intervals(log_file_path, model_path, interval_minutes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "\n",
    "# def generate_signatures_from_log(df):\n",
    "#     \"\"\"\n",
    "#     Generate signatures from the DataFrame of logs in the form [signature, frequency].\n",
    "    \n",
    "#     Parameters:\n",
    "#         df (DataFrame): DataFrame containing the log data.\n",
    "        \n",
    "#     Returns:\n",
    "#         list: A list of signatures and their frequencies.\n",
    "#     \"\"\"\n",
    "#     # Initialize a dictionary to hold the frequency of each signature\n",
    "#     signature_counts = {}\n",
    "\n",
    "#     # Create signatures based on specified criteria\n",
    "#     for _, row in df.iterrows():\n",
    "#         # Extracting necessary fields\n",
    "#         id_resp_p = row[5]  # id.resp_p\n",
    "#         proto = row[6]  # proto\n",
    "#         service = row[7]  # service\n",
    "#         orig_bytes = row[9]  # orig_bytes\n",
    "#         resp_bytes = row[10]  # resp_bytes\n",
    "#         conn_state = row[11]  # conn_state\n",
    "        \n",
    "#         # Skip service if it's '0', '-', or empty\n",
    "#         service = service if service not in ['0', '-', ''] else ''\n",
    "\n",
    "#         # Replace '-' or '0' with '0' for orig_bytes and resp_bytes\n",
    "#         orig_bytes = orig_bytes if orig_bytes not in ['-', '0'] else '0'\n",
    "#         resp_bytes = resp_bytes if resp_bytes not in ['-', '0'] else '0'\n",
    "\n",
    "#         # Create the signature string\n",
    "#         signature = f\"{id_resp_p}{proto}{service}{orig_bytes}{resp_bytes}{conn_state}\"\n",
    "        \n",
    "#         # Count frequencies\n",
    "#         if signature in signature_counts:\n",
    "#             signature_counts[signature] += 1\n",
    "#         else:\n",
    "#             signature_counts[signature] = 1\n",
    "\n",
    "#     # Prepare the output list in the desired format: [[signature, frequency], ...]\n",
    "#     output_list = [[signature, frequency] for signature, frequency in signature_counts.items()]\n",
    "\n",
    "#     return output_list  # Return a list instead of a DataFrame for the output\n",
    "\n",
    "# def process_log_in_intervals(file_path, model_path, interval_minutes=5):\n",
    "#     try:\n",
    "#         # Load the log file into a DataFrame while skipping the first 8 rows\n",
    "#         df = pd.read_csv(file_path, delim_whitespace=True, header=None, na_filter=False,\n",
    "#                          on_bad_lines='skip', skiprows=8, low_memory=False)\n",
    "\n",
    "#         # Replace non-numeric values in the timestamp column with NaN and convert\n",
    "#         df[0] = pd.to_numeric(df[0], errors='coerce')  # Convert timestamps to numeric\n",
    "        \n",
    "#         # Drop rows with NaN values in critical columns\n",
    "#         df.dropna(subset=[0, 3, 5, 6, 14, 16, 17], inplace=True)  # Ensure key columns are not NaN\n",
    "\n",
    "#         # Convert timestamp to datetime\n",
    "#         df[0] = pd.to_datetime(df[0], errors='coerce')  # Convert to datetime\n",
    "\n",
    "#         # Check for missing values after conversion\n",
    "#         df.dropna(subset=[0], inplace=True)\n",
    "\n",
    "#         # Load the trained model\n",
    "#         model_data = joblib.load(model_path)\n",
    "#         clf = model_data['model']\n",
    "#         signature_list = model_data['signature_list']\n",
    "\n",
    "#         # Define time intervals\n",
    "#         start_time = df[0].min()\n",
    "#         end_time = df[0].max()\n",
    "#         current_time = start_time\n",
    "\n",
    "#         all_predictions = {}  # Dictionary to store predictions for all intervals\n",
    "\n",
    "#         print(f\"Processing intervals from {start_time} to {end_time}\")  # Debugging line\n",
    "\n",
    "#         while current_time < end_time:\n",
    "#             # Create time range for the current interval\n",
    "#             next_time = current_time + pd.Timedelta(minutes=interval_minutes)\n",
    "\n",
    "#             # Filter the DataFrame for the current time range\n",
    "#             filtered_df = df[(df[0] >= current_time) & (df[0] < next_time)]\n",
    "\n",
    "#             # Debugging output to see the current time range and filtered results\n",
    "#             print(f\"Current Time: {current_time}, Next Time: {next_time}, Filtered Count: {len(filtered_df)}\")\n",
    "\n",
    "#             if not filtered_df.empty:\n",
    "#                 # Generate signatures from the filtered DataFrame\n",
    "#                 signatures = generate_signatures_from_log(filtered_df)\n",
    "\n",
    "#                 # Debugging: Print generated signatures\n",
    "#                 print(f\"Generated Signatures: {signatures}\")\n",
    "\n",
    "#                 # Prepare the signature matrix for prediction\n",
    "#                 X_matrix = np.zeros((1, len(signature_list)), dtype=int)\n",
    "\n",
    "#                 # Populate the matrix with frequencies\n",
    "#                 for sig, frequency in signatures:\n",
    "#                     if sig in signature_list:\n",
    "#                         sig_idx = signature_list.index(sig)  # Get the index for the signature\n",
    "#                         X_matrix[0, sig_idx] += frequency  # Update the corresponding cell with the frequency\n",
    "\n",
    "#                 # Make predictions\n",
    "#                 predictions = clf.predict(X_matrix)\n",
    "\n",
    "#                 # Check if predictions are an array (e.g., multiple classes)\n",
    "#                 if isinstance(predictions, np.ndarray):\n",
    "#                     predictions = predictions.tolist()\n",
    "\n",
    "#                 # Debugging: Print predictions\n",
    "#                 print(f\"Predictions: {predictions}\")\n",
    "\n",
    "#                 # Store the prediction in the dictionary with the current time interval as the key\n",
    "#                 time_interval_key = f\"{current_time} - {next_time}\"\n",
    "#                 if time_interval_key in all_predictions:\n",
    "#                     all_predictions[time_interval_key].extend(predictions)  # Append new predictions\n",
    "#                 else:\n",
    "#                     all_predictions[time_interval_key] = predictions  # Store new prediction\n",
    "\n",
    "#             # Move to the next time interval\n",
    "#             current_time = next_time  # Update to the next interval\n",
    "\n",
    "#         # Print all predictions after processing all intervals\n",
    "#         print(\"All Predictions:\")\n",
    "#         for time_interval, predicted_labels in all_predictions.items():\n",
    "#             # Convert to a set to remove duplicates, then back to a list and sort\n",
    "#             unique_labels = sorted(set(predicted_labels))  \n",
    "#             print(f\"Time Interval: {time_interval} : {unique_labels}\")\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path} or {model_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "\n",
    "# def print_log_duration(file_path, nrows=None):\n",
    "#     \"\"\"\n",
    "#     Print the start time, end time, and duration of the dataset.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Read the log file into a DataFrame\n",
    "#         df = pd.read_table(file_path, delim_whitespace=True, header=None, usecols=[0], skiprows=8, nrows=nrows)\n",
    "        \n",
    "#         # Convert the first column (timestamp) to datetime\n",
    "#         df.columns = ['Timestamp']\n",
    "#         df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce', unit='s')\n",
    "        \n",
    "#         # Calculate the duration by subtracting the minimum timestamp from the maximum timestamp\n",
    "#         min_timestamp = df['Timestamp'].min()\n",
    "#         max_timestamp = df['Timestamp'].max()\n",
    "#         duration = max_timestamp - min_timestamp\n",
    "        \n",
    "#         # Print the start time, end time, and duration\n",
    "#         print(f\"Start Time: {min_timestamp}\")\n",
    "#         print(f\"End Time:   {max_timestamp}\")\n",
    "#         print(f\"Duration: {duration}\")\n",
    "        \n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "\n",
    "# def generate_signatures_from_log(df):\n",
    "#     \"\"\"\n",
    "#     Generate signatures from the DataFrame of logs in the form [signature, frequency].\n",
    "#     \"\"\"\n",
    "#     signature_counts = {}\n",
    "    \n",
    "#     for _, row in df.iterrows():\n",
    "#         id_resp_p = row[5]\n",
    "#         proto = row[6]\n",
    "#         service = row[7]\n",
    "#         orig_bytes = row[9]\n",
    "#         resp_bytes = row[10]\n",
    "#         conn_state = row[11]\n",
    "        \n",
    "#         service = service if service not in ['0', '-', ''] else ''\n",
    "#         orig_bytes = orig_bytes if orig_bytes not in ['-', '0'] else '0'\n",
    "#         resp_bytes = resp_bytes if resp_bytes not in ['-', '0'] else '0'\n",
    "\n",
    "#         signature = f\"{id_resp_p}{proto}{service}{orig_bytes}{resp_bytes}{conn_state}\"\n",
    "\n",
    "#         if signature in signature_counts:\n",
    "#             signature_counts[signature] += 1\n",
    "#         else:\n",
    "#             signature_counts[signature] = 1\n",
    "\n",
    "#     output_list = [[signature, frequency] for signature, frequency in signature_counts.items()]\n",
    "#     return output_list\n",
    "\n",
    "# def process_log_in_intervals(file_path, model_path, interval_minutes=5):\n",
    "#     try:\n",
    "#         # Print the duration of the log file\n",
    "#         print(\"Scanning dataset duration:\")\n",
    "#         print_log_duration(file_path)  # Using the provided function to print duration\n",
    "\n",
    "#         # Load the log data into a DataFrame\n",
    "#         df = pd.read_csv(file_path, delim_whitespace=True, header=None, na_filter=False,\n",
    "#                          on_bad_lines='skip', skiprows=8, low_memory=False)\n",
    "\n",
    "#         # Convert the first column to datetime (timestamps)\n",
    "#         df[0] = pd.to_numeric(df[0], errors='coerce')\n",
    "#         df.dropna(subset=[0, 3, 5, 6, 14, 16, 17], inplace=True)\n",
    "#         df[0] = pd.to_datetime(df[0], errors='coerce', unit='s')  # Ensure the timestamps are correct\n",
    "#         df.dropna(subset=[0], inplace=True)\n",
    "\n",
    "#         # Load the model and signature list\n",
    "#         model_data = joblib.load(model_path)\n",
    "#         clf = model_data['model']\n",
    "#         signature_list = model_data['signature_list']\n",
    "\n",
    "#         # Define time intervals\n",
    "#         start_time = df[0].min()\n",
    "#         end_time = df[0].max()\n",
    "#         current_time = start_time\n",
    "\n",
    "#         # Print the range of timestamps\n",
    "#         print(f\"Data Range: Min Timestamp = {start_time}, Max Timestamp = {end_time}\")\n",
    "\n",
    "#         # Process data in 5-minute intervals\n",
    "#         interval_duration = pd.Timedelta(minutes=interval_minutes)\n",
    "#         interval_count = 1  # Initialize interval counter\n",
    "\n",
    "#         # Store row counts for each interval\n",
    "#         interval_row_counts = []\n",
    "\n",
    "#         while current_time < end_time:\n",
    "#             next_time = current_time + interval_duration\n",
    "\n",
    "#             # Filter data for the current 5-minute interval\n",
    "#             filtered_df = df[(df[0] >= current_time) & (df[0] < next_time)]\n",
    "            \n",
    "#             # Count and print the number of rows in this interval\n",
    "#             num_rows = len(filtered_df)\n",
    "#             interval_row_counts.append((current_time, next_time, num_rows))  # Store the interval count\n",
    "\n",
    "#             if num_rows > 0:  # Process only if there are rows in the interval\n",
    "#                 # Generate signatures from the filtered logs\n",
    "#                 signatures = generate_signatures_from_log(filtered_df)\n",
    "\n",
    "#                 # Prepare the feature matrix (X_matrix) for prediction\n",
    "#                 X_matrix = np.zeros((1, len(signature_list)), dtype=int)\n",
    "\n",
    "#                 # Map signatures to the matrix\n",
    "#                 for sig, frequency in signatures:\n",
    "#                     if sig in signature_list:\n",
    "#                         sig_idx = signature_list.index(sig)\n",
    "#                         X_matrix[0, sig_idx] += frequency\n",
    "\n",
    "#                 # Make predictions for the current interval\n",
    "#                 predictions = clf.predict(X_matrix)\n",
    "\n",
    "#                 # Ensure predictions are in list format\n",
    "#                 if isinstance(predictions, np.ndarray):\n",
    "#                     predictions = predictions.tolist()\n",
    "\n",
    "#                 # Print the predictions for the current interval\n",
    "#                 print(f\"Time Interval {interval_count}: {current_time} - {next_time} : {predictions}\")\n",
    "\n",
    "#             # Move to the next 5-minute interval\n",
    "#             current_time = next_time\n",
    "#             interval_count += 1  # Increment interval counter\n",
    "\n",
    "#         # After processing, print the row counts for each interval\n",
    "#         print(\"\\nRow counts for each interval:\")\n",
    "#         for start, end, count in interval_row_counts:\n",
    "#             print(f\"From {start} to {end}: {count} packets\")\n",
    "\n",
    "#         print(\"Processing complete.\")\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path} or {model_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "\n",
    "# # Example usage\n",
    "# # log_file_path = \"path_to_your_log_file\"\n",
    "# # model_path = \"path_to_your_model_file\"\n",
    "# # process_log_in_intervals(log_file_path, model_path, interval_minutes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "\n",
    "# def print_log_duration(file_path, nrows=None):\n",
    "#     \"\"\"\n",
    "#     Print the start time, end time, and duration of the dataset.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Read the log file into a DataFrame\n",
    "#         df = pd.read_table(file_path, delim_whitespace=True, header=None, usecols=[0], skiprows=8, nrows=nrows)\n",
    "        \n",
    "#         # Convert the first column (timestamp) to datetime\n",
    "#         df.columns = ['Timestamp']\n",
    "#         df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce', unit='s')\n",
    "        \n",
    "#         # Calculate the duration by subtracting the minimum timestamp from the maximum timestamp\n",
    "#         min_timestamp = df['Timestamp'].min()\n",
    "#         max_timestamp = df['Timestamp'].max()\n",
    "#         duration = max_timestamp - min_timestamp\n",
    "        \n",
    "#         # Print the start time, end time, and duration\n",
    "#         print(f\"Start Time: {min_timestamp}\")\n",
    "#         print(f\"End Time:   {max_timestamp}\")\n",
    "#         print(f\"Duration: {duration}\")\n",
    "        \n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "\n",
    "# def generate_signatures_from_log(df):\n",
    "#     \"\"\"\n",
    "#     Generate signatures from the DataFrame of logs in the form [signature, frequency].\n",
    "#     \"\"\"\n",
    "#     signature_counts = {}\n",
    "    \n",
    "#     for _, row in df.iterrows():\n",
    "#         id_resp_p = row[5]\n",
    "#         proto = row[6]\n",
    "#         service = row[7]\n",
    "#         orig_bytes = row[9]\n",
    "#         resp_bytes = row[10]\n",
    "#         conn_state = row[11]\n",
    "        \n",
    "#         service = service if service not in ['0', '-', ''] else ''\n",
    "#         orig_bytes = orig_bytes if orig_bytes not in ['-', '0'] else '0'\n",
    "#         resp_bytes = resp_bytes if resp_bytes not in ['-', '0'] else '0'\n",
    "\n",
    "#         signature = f\"{id_resp_p}{proto}{service}{orig_bytes}{resp_bytes}{conn_state}\"\n",
    "\n",
    "#         if signature in signature_counts:\n",
    "#             signature_counts[signature] += 1\n",
    "#         else:\n",
    "#             signature_counts[signature] = 1\n",
    "\n",
    "#     output_list = [[signature, frequency] for signature, frequency in signature_counts.items()]\n",
    "#     return output_list\n",
    "\n",
    "# def process_log_in_intervals(file_path, model_path, interval_minutes=1):\n",
    "#     try:\n",
    "#         # Print the duration of the log file\n",
    "#         print(\"Scanning dataset duration:\")\n",
    "#         print_log_duration(file_path)  # Using the provided function to print duration\n",
    "\n",
    "#         # Load the log data into a DataFrame\n",
    "#         df = pd.read_csv(file_path, delim_whitespace=True, header=None, na_filter=False,\n",
    "#                          on_bad_lines='skip', skiprows=8, low_memory=False)\n",
    "\n",
    "#         # Convert the first column to datetime (timestamps)\n",
    "#         df[0] = pd.to_numeric(df[0], errors='coerce')\n",
    "#         df.dropna(subset=[0, 3, 5, 6, 14, 16, 17], inplace=True)\n",
    "#         df[0] = pd.to_datetime(df[0], errors='coerce', unit='s')  # Ensure the timestamps are correct\n",
    "#         df.dropna(subset=[0], inplace=True)\n",
    "\n",
    "#         # Load the model and signature list\n",
    "#         model_data = joblib.load(model_path)\n",
    "#         clf = model_data['model']\n",
    "#         signature_list = model_data['signature_list']\n",
    "\n",
    "#         # Define time intervals\n",
    "#         start_time = df[0].min()\n",
    "#         end_time = df[0].max()\n",
    "#         current_time = start_time\n",
    "\n",
    "#         # Print the range of timestamps\n",
    "#         print(f\"Data Range: Min Timestamp = {start_time}, Max Timestamp = {end_time}\")\n",
    "\n",
    "#         # Process data in 5-minute intervals\n",
    "#         interval_duration = pd.Timedelta(minutes=interval_minutes)\n",
    "#         interval_count = 1  # Initialize interval counter\n",
    "\n",
    "#         # Store row counts for each interval\n",
    "#         interval_row_counts = []\n",
    "\n",
    "#         # Label mapping\n",
    "#         label_mapping = {\n",
    "#             0: 'Attack',\n",
    "#             1: 'Benign',\n",
    "#             2: 'C&C',\n",
    "#             3: 'C&C-HeartBeat',\n",
    "#             4: 'DDoS',\n",
    "#             5: 'Okiru',\n",
    "#             6: 'PortScan',\n",
    "#             7: 'PortScan-Attack'\n",
    "#         }\n",
    "\n",
    "#         while current_time < end_time:\n",
    "#             next_time = current_time + interval_duration\n",
    "\n",
    "#             # Filter data for the current 5-minute interval\n",
    "#             filtered_df = df[(df[0] >= current_time) & (df[0] < next_time)]\n",
    "            \n",
    "#             # Count the number of rows in this interval\n",
    "#             num_rows = len(filtered_df)\n",
    "#             interval_row_counts.append((current_time, next_time, num_rows))  # Store the interval count\n",
    "\n",
    "#             if num_rows > 0:  # Process only if there are rows in the interval\n",
    "#                 # Generate signatures from the filtered logs\n",
    "#                 signatures = generate_signatures_from_log(filtered_df)\n",
    "\n",
    "#                 # Prepare the feature matrix (X_matrix) for prediction\n",
    "#                 X_matrix = np.zeros((1, len(signature_list)), dtype=int)\n",
    "\n",
    "#                 # Map signatures to the matrix\n",
    "#                 for sig, frequency in signatures:\n",
    "#                     if sig in signature_list:\n",
    "#                         sig_idx = signature_list.index(sig)\n",
    "#                         X_matrix[0, sig_idx] += frequency\n",
    "\n",
    "#                 # Make predictions for the current interval\n",
    "#                 predictions = clf.predict(X_matrix)\n",
    "\n",
    "#                 # Convert numerical predictions to corresponding labels\n",
    "#                 predicted_labels = [label_mapping[pred] for pred in predictions]\n",
    "\n",
    "#                 # Print the output in the desired format\n",
    "#                 signatures_output = ', '.join([f'[{sig}, {freq}]' for sig, freq in signatures])\n",
    "#                 print(f\"Interval {interval_count}: {signatures_output}\")\n",
    "#                 print(f\">> prediction: {predicted_labels}\")\n",
    "\n",
    "#             # Move to the next 5-minute interval\n",
    "#             current_time = next_time\n",
    "#             interval_count += 1  # Increment interval counter\n",
    "\n",
    "#         print(\"Processing complete.\")\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path} or {model_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with DCT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version 1\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "\n",
    "# # Function to generate signatures from a DataFrame\n",
    "# def generate_signatures_from_log(df):\n",
    "#     signature_counts = {}\n",
    "    \n",
    "#     for _, row in df.iterrows():\n",
    "#         id_resp_p = row[5]\n",
    "#         proto = row[6]\n",
    "#         service = row[7]\n",
    "#         orig_bytes = row[9]\n",
    "#         resp_bytes = row[10]\n",
    "#         conn_state = row[11]\n",
    "        \n",
    "#         service = service if service not in ['0', '-', ''] else ''\n",
    "#         orig_bytes = orig_bytes if orig_bytes not in ['-', '0'] else '0'\n",
    "#         resp_bytes = resp_bytes if resp_bytes not in ['-', '0'] else '0'\n",
    "\n",
    "#         signature = f\"{id_resp_p}{proto}{service}{orig_bytes}{resp_bytes}{conn_state}\"\n",
    "\n",
    "#         if signature in signature_counts:\n",
    "#             signature_counts[signature] += 1\n",
    "#         else:\n",
    "#             signature_counts[signature] = 1\n",
    "\n",
    "#     output_list = [[signature, frequency] for signature, frequency in signature_counts.items()]\n",
    "#     return output_list\n",
    "\n",
    "# def process_log_in_intervals(file_path, model_path, interval_minutes=5):\n",
    "#     try:\n",
    "#         # Print the duration of the log file\n",
    "#         print(\"Scanning dataset duration:\")\n",
    "#         # print_log_duration(file_path)  # Using the provided function to print duration\n",
    "\n",
    "#         # Load the log data into a DataFrame\n",
    "#         df = pd.read_csv(file_path, delim_whitespace=True, header=None, na_filter=False,\n",
    "#                          on_bad_lines='skip', skiprows=8, low_memory=False)\n",
    "\n",
    "#         # Convert the first column to datetime (timestamps)\n",
    "#         df[0] = pd.to_numeric(df[0], errors='coerce')\n",
    "#         df.dropna(subset=[0, 3, 5, 6, 14, 16, 17], inplace=True)\n",
    "#         df[0] = pd.to_datetime(df[0], errors='coerce', unit='s')  # Ensure the timestamps are correct\n",
    "#         df.dropna(subset=[0], inplace=True)\n",
    "\n",
    "#         # Load the model and signature list\n",
    "#         model_data = joblib.load(model_path)\n",
    "#         clf = model_data['model']\n",
    "#         signature_list = model_data['signature_list']\n",
    "\n",
    "#         # Define time intervals\n",
    "#         start_time = df[0].min()\n",
    "#         end_time = df[0].max()\n",
    "#         current_time = start_time\n",
    "\n",
    "#         # Print the range of timestamps\n",
    "#         print(f\"Data Range: Min Timestamp = {start_time}, Max Timestamp = {end_time}\")\n",
    "\n",
    "#         # Process data in 5-minute intervals\n",
    "#         interval_duration = pd.Timedelta(minutes=interval_minutes)\n",
    "#         interval_count = 1  # Initialize interval counter\n",
    "\n",
    "#         # Label mapping\n",
    "#         label_mapping = {\n",
    "#             0: 'Attack',\n",
    "#             1: 'C&C',\n",
    "#             2: 'C&C-HeartBeat',\n",
    "#             3: 'C&C-Torii',\n",
    "#             4: 'DDoS',\n",
    "#             5: 'Okiru',\n",
    "#             6: 'PortScan',\n",
    "#             7: 'PortScan-Attack'\n",
    "#         }\n",
    "\n",
    "#         while current_time < end_time:\n",
    "#             next_time = current_time + interval_duration\n",
    "\n",
    "#             # Filter data for the current 5-minute interval\n",
    "#             filtered_df = df[(df[0] >= current_time) & (df[0] < next_time)]\n",
    "\n",
    "#             if not filtered_df.empty:  # Process only if there are rows in the interval\n",
    "#                 # Generate signatures from the filtered logs\n",
    "#                 signatures = generate_signatures_from_log(filtered_df)\n",
    "\n",
    "#                 # Prepare the feature matrix (X_matrix) for prediction\n",
    "#                 X_matrix = np.zeros((len(signatures), len(signature_list)), dtype=int)\n",
    "\n",
    "#                 # Map signatures to the matrix\n",
    "#                 for sig, frequency in signatures:\n",
    "#                     if sig in signature_list:\n",
    "#                         sig_idx = signature_list.index(sig)\n",
    "#                         # Update the matrix for this signature\n",
    "#                         X_matrix[signatures.index([sig, frequency]), sig_idx] += frequency\n",
    "\n",
    "#                 # Make predictions for all signatures in the current interval\n",
    "#                 predictions = clf.predict(X_matrix)\n",
    "\n",
    "#                 # Convert numerical predictions to corresponding labels\n",
    "#                 predicted_labels = [label_mapping[pred] for pred in predictions]\n",
    "\n",
    "#                 # Remove duplicates from predicted_labels\n",
    "#                 unique_predicted_labels = list(set(predicted_labels))\n",
    "\n",
    "#                 # Print the output in the desired format\n",
    "#                 signatures_output = ', '.join([f'[{sig}, {freq}]' for sig, freq in signatures])\n",
    "#                 # print(f\"Interval {interval_count}: {signatures_output}\")\n",
    "#                 print(f\"Interval {interval_count}: {unique_predicted_labels}\")\n",
    "#                 # print(f\">> Unique predictions: {unique_predicted_labels}\")\n",
    "\n",
    "#             # Move to the next 5-minute interval\n",
    "#             current_time = next_time\n",
    "#             interval_count += 1  # Increment interval counter\n",
    "\n",
    "#         print(\"Processing complete.\")\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path} or {model_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with all model 7 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version 2  Can predict all dataset & all models\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "\n",
    "# # Function to generate signatures from a DataFrame\n",
    "# def generate_signatures_from_log(df):\n",
    "#     signature_counts = {}\n",
    "    \n",
    "#     for _, row in df.iterrows():\n",
    "#         id_resp_p = row[5]\n",
    "#         proto = row[6]\n",
    "#         service = row[7]\n",
    "#         orig_bytes = row[9]\n",
    "#         resp_bytes = row[10]\n",
    "#         conn_state = row[11]\n",
    "        \n",
    "#         # Clean up service and byte values\n",
    "#         service = service if service not in ['0', '-', ''] else ''\n",
    "#         orig_bytes = orig_bytes if orig_bytes not in ['-', '0'] else '0'\n",
    "#         resp_bytes = resp_bytes if resp_bytes not in ['-', '0'] else '0'\n",
    "\n",
    "#         # Create the signature\n",
    "#         signature = f\"{id_resp_p}{proto}{service}{orig_bytes}{resp_bytes}{conn_state}\"\n",
    "\n",
    "#         # Count occurrences of each signature\n",
    "#         signature_counts[signature] = signature_counts.get(signature, 0) + 1\n",
    "\n",
    "#     output_list = [[signature, frequency] for signature, frequency in signature_counts.items()]\n",
    "#     return output_list\n",
    "\n",
    "# def process_log_in_intervals(file_path, model_path, interval_minutes):  # Changed to 1 minute\n",
    "#     try:\n",
    "#         # Load the log data into a DataFrame\n",
    "#         df = pd.read_csv(file_path, delim_whitespace=True, header=None, na_filter=False,\n",
    "#                          on_bad_lines='skip', skiprows=8, low_memory=False)\n",
    "\n",
    "#         # Convert the first column to datetime (timestamps)\n",
    "#         df[0] = pd.to_numeric(df[0], errors='coerce')\n",
    "#         df.dropna(subset=[0, 3, 5, 6, 14, 16, 17], inplace=True)\n",
    "#         df[0] = pd.to_datetime(df[0], errors='coerce', unit='s')  # Ensure the timestamps are correct\n",
    "#         df.dropna(subset=[0], inplace=True)\n",
    "\n",
    "#         # Load the model and signature list\n",
    "#         model_data = joblib.load(model_path)\n",
    "#         clf = model_data['model']\n",
    "#         signature_list = model_data['signature_list']\n",
    "\n",
    "#         # Define time intervals\n",
    "#         start_time = df[0].min()\n",
    "#         end_time = df[0].max()\n",
    "#         current_time = start_time\n",
    "\n",
    "#         # Print the range of timestamps\n",
    "#         print(f\"Data Range: Min Timestamp = {start_time}, Max Timestamp = {end_time}\")\n",
    "\n",
    "#         # Process data in defined intervals\n",
    "#         interval_duration = pd.Timedelta(minutes=interval_minutes)\n",
    "#         interval_count = 1  # Initialize interval counter\n",
    "\n",
    "#         # Label mapping\n",
    "#         label_mapping = {\n",
    "#             0: 'Attack',\n",
    "#             1: 'C&C',\n",
    "#             2: 'C&C-HeartBeat',\n",
    "#             3: 'C&C-Torii',\n",
    "#             4: 'DDoS',\n",
    "#             5: 'Okiru',\n",
    "#             6: 'PortScan',\n",
    "#             7: 'PortScan-Attack'\n",
    "#         }\n",
    "\n",
    "#         # List to capture predictions over time\n",
    "#         predictions_over_time = []\n",
    "\n",
    "#         while current_time < end_time:\n",
    "#             next_time = current_time + interval_duration\n",
    "\n",
    "#             # Filter data for the current interval\n",
    "#             filtered_df = df[(df[0] >= current_time) & (df[0] < next_time)]\n",
    "\n",
    "#             if not filtered_df.empty:  # Process only if there are rows in the interval\n",
    "#                 # Generate signatures from the filtered logs\n",
    "#                 signatures = generate_signatures_from_log(filtered_df)\n",
    "\n",
    "#                 # Prepare the feature matrix (X_matrix) for prediction\n",
    "#                 X_matrix = np.zeros((len(signatures), len(signature_list)), dtype=int)\n",
    "\n",
    "#                 # Map signatures to the matrix\n",
    "#                 for i, (sig, frequency) in enumerate(signatures):\n",
    "#                     if sig in signature_list:\n",
    "#                         sig_idx = signature_list.index(sig)\n",
    "#                         # Update the matrix for this signature\n",
    "#                         X_matrix[i, sig_idx] = frequency  # Use i to index X_matrix\n",
    "\n",
    "#                 # Make predictions for all signatures in the current interval\n",
    "#                 predictions = clf.predict(X_matrix)\n",
    "\n",
    "#                 # Convert numerical predictions to corresponding labels\n",
    "#                 predicted_labels = [label_mapping[pred] for pred in predictions]\n",
    "\n",
    "#                 # Store predictions for the current time interval\n",
    "#                 predictions_over_time.append((current_time, predicted_labels))\n",
    "\n",
    "#                 # Print the unique predicted labels for the interval\n",
    "#                 unique_predicted_labels = list(set(predicted_labels))\n",
    "#                 # print(f\"Interval {interval_count}: {unique_predicted_labels}\")\n",
    "\n",
    "#                 # Count frequency of each prediction in this interval\n",
    "#                 prediction_counts = {label: predicted_labels.count(label) for label in unique_predicted_labels}\n",
    "#                 print(f\"Interval {interval_count}: \\t{current_time}\")\n",
    "#                 print(f\"\\t\\t{prediction_counts}\")\n",
    "\n",
    "#             # Move to the next interval\n",
    "#             current_time = next_time\n",
    "#             interval_count += 1  # Increment interval counter\n",
    "\n",
    "#         print(\"Processing complete.\")\n",
    "\n",
    "#         # After processing, print predictions for each time interval\n",
    "#         # for time, preds in predictions_over_time:\n",
    "#         #     print(f\"Time: {time}, Predictions: {preds}\")\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path} or {model_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import psutil  # Library for monitoring system resources\n",
    "import time\n",
    "\n",
    "# Function to generate signatures from a DataFrame\n",
    "def generate_signatures_from_log(df):\n",
    "    signature_counts = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        id_resp_p = row[5]\n",
    "        proto = row[6]\n",
    "        service = row[7]\n",
    "        orig_bytes = row[9]\n",
    "        resp_bytes = row[10]\n",
    "        conn_state = row[11]\n",
    "\n",
    "        # Clean up service and byte values\n",
    "        service = service if service not in ['0', '-', ''] else ''\n",
    "        orig_bytes = orig_bytes if orig_bytes not in ['-', '0'] else '0'\n",
    "        resp_bytes = resp_bytes if resp_bytes not in ['-', '0'] else '0'\n",
    "\n",
    "        # Create the signature\n",
    "        signature = f\"{id_resp_p}{proto}{service}{orig_bytes}{resp_bytes}{conn_state}\"\n",
    "\n",
    "        # Count occurrences of each signature\n",
    "        signature_counts[signature] = signature_counts.get(signature, 0) + 1\n",
    "\n",
    "    output_list = [[signature, frequency] for signature, frequency in signature_counts.items()]\n",
    "    return output_list\n",
    "\n",
    "# Function to monitor and print resource usage for each batch\n",
    "def measure_resource_usage(initial_memory, initial_cpu):\n",
    "    process = psutil.Process()\n",
    "    current_memory = process.memory_info().rss / (1024 * 1024)  # Memory usage in MB\n",
    "    current_cpu = psutil.cpu_percent(interval=1)  # CPU usage in percentage\n",
    "\n",
    "    # Calculate the difference (resource used by the current batch)\n",
    "    memory_used = current_memory - initial_memory\n",
    "    cpu_used = current_cpu - initial_cpu\n",
    "\n",
    "    return memory_used, cpu_used\n",
    "\n",
    "def process_log_in_intervals(file_path, model_path, interval_minutes=5, batch_size=5):\n",
    "    try:\n",
    "        # Load the model and signature list\n",
    "        model_data = joblib.load(model_path)\n",
    "        clf = model_data['model']\n",
    "        signature_list = model_data['signature_list']\n",
    "\n",
    "        # Define time intervals in seconds\n",
    "        interval_duration = pd.Timedelta(minutes=interval_minutes)\n",
    "\n",
    "        # Chunk size to read the file in batches\n",
    "        chunk_size = 5000  # Adjust based on your system's RAM capacity\n",
    "\n",
    "        # Read the log data in chunks\n",
    "        df_iterator = pd.read_csv(file_path, delim_whitespace=True, header=None, na_filter=False,\n",
    "                                  on_bad_lines='skip', skiprows=8, low_memory=False, chunksize=chunk_size)\n",
    "\n",
    "        # Initialize variables to track current interval\n",
    "        current_time = None\n",
    "        next_time = None\n",
    "        interval_count = 1\n",
    "\n",
    "        # Initialize variables for batch processing\n",
    "        interval_buffer = []\n",
    "        time_buffer = []\n",
    "        batch_counter = 0\n",
    "\n",
    "        # Label mapping\n",
    "        label_mapping = {\n",
    "            0: 'Attack',\n",
    "            1: 'C&C',\n",
    "            2: 'C&C-HeartBeat',\n",
    "            3: 'C&C-Torii',\n",
    "            4: 'DDoS',\n",
    "            5: 'Okiru',\n",
    "            6: 'PortScan',\n",
    "            7: 'PortScan-Attack'\n",
    "        }\n",
    "\n",
    "        # Measure the initial resource usage before starting processing\n",
    "        print(\"Measuring Initial Resource Usage...\")\n",
    "        process = psutil.Process()                                                                                         # Get the current process\n",
    "        initial_memory_usage = process.memory_info().rss / (1024 * 1024)                                                   # Initial memory usage in MB\n",
    "        # initial_memory_usage = process.memory_info().rss / 1024                                                          # Initial memory usage in KB\n",
    "        initial_cpu_usage = psutil.cpu_percent(interval=1)                                                                 # Initial CPU usage over 1 second\n",
    "        print(f\"Initial Memory Usage: {initial_memory_usage:.2f} MB, Initial CPU Usage: {initial_cpu_usage:.2f}%\\n\")\n",
    "\n",
    "        start_time = time.time()  # Start time to measure execution duration\n",
    "\n",
    "        for chunk in df_iterator:\n",
    "            # Preprocess the chunk\n",
    "            chunk[0] = pd.to_numeric(chunk[0], errors='coerce')\n",
    "            chunk.dropna(subset=[0, 3, 5, 6, 14, 16, 17], inplace=True)\n",
    "            chunk[0] = pd.to_datetime(chunk[0], errors='coerce', unit='s')  # Convert timestamp to datetime\n",
    "            chunk.dropna(subset=[0], inplace=True)\n",
    "\n",
    "            if current_time is None:\n",
    "                current_time = chunk[0].min()\n",
    "                next_time = current_time + interval_duration\n",
    "\n",
    "            # Process the chunk in intervals\n",
    "            while current_time < chunk[0].max():\n",
    "                # Filter data for the current interval\n",
    "                interval_df = chunk[(chunk[0] >= current_time) & (chunk[0] < next_time)]\n",
    "\n",
    "                if not interval_df.empty:\n",
    "                    # Generate signatures from the filtered logs\n",
    "                    signatures = generate_signatures_from_log(interval_df)\n",
    "\n",
    "                    # Prepare the feature matrix (X_matrix) for prediction\n",
    "                    X_matrix = np.zeros((len(signatures), len(signature_list)), dtype=int)\n",
    "\n",
    "                    # Map signatures to the matrix\n",
    "                    for i, (sig, frequency) in enumerate(signatures):\n",
    "                        if sig in signature_list:\n",
    "                            sig_idx = signature_list.index(sig)\n",
    "                            # Update the matrix for this signature\n",
    "                            X_matrix[i, sig_idx] = frequency  # Use i to index X_matrix\n",
    "\n",
    "                    # Add to interval buffer\n",
    "                    interval_buffer.append(X_matrix)\n",
    "                    time_buffer.append(current_time)\n",
    "                    batch_counter += 1\n",
    "\n",
    "                # If batch size is reached, make predictions for all intervals in the batch\n",
    "                if batch_counter >= batch_size:\n",
    "                    for X_mat, interval_time in zip(interval_buffer, time_buffer):\n",
    "                        # Make predictions for all signatures in the current batch of intervals\n",
    "                        predictions = clf.predict(X_mat)\n",
    "\n",
    "                        # Convert numerical predictions to corresponding labels\n",
    "                        predicted_labels = [label_mapping[pred] for pred in predictions]\n",
    "\n",
    "                        # Print the unique predicted labels for the interval\n",
    "                        unique_predicted_labels = list(set(predicted_labels))\n",
    "                        prediction_counts = {label: predicted_labels.count(label) for label in unique_predicted_labels}\n",
    "                        print(f\"Interval {interval_count}: {interval_time}\")\n",
    "                        print(f\"\\tPredictions: {prediction_counts}\")\n",
    "\n",
    "                        interval_count += 1\n",
    "\n",
    "                    # Clear buffers after processing batch\n",
    "                    interval_buffer = []\n",
    "                    time_buffer = []\n",
    "                    batch_counter = 0\n",
    "\n",
    "                    # Measure resource usage for the current batch and calculate the difference\n",
    "                    memory_used, cpu_used = measure_resource_usage(initial_memory_usage, initial_cpu_usage)\n",
    "                    # print(f\"\\nBatch {interval_count // batch_size} Resource Usage:\")\n",
    "                    print(f\"------- Memory Used: {memory_used:.2f} MB\\n------- CPU Used: {cpu_used:.2f}%\\n\")\n",
    "\n",
    "                # Move to the next interval\n",
    "                current_time = next_time\n",
    "                next_time = current_time + interval_duration\n",
    "\n",
    "        # Process any remaining intervals in the buffer\n",
    "        if interval_buffer:\n",
    "            for X_mat, interval_time in zip(interval_buffer, time_buffer):\n",
    "                predictions = clf.predict(X_mat)\n",
    "                predicted_labels = [label_mapping[pred] for pred in predictions]\n",
    "                unique_predicted_labels = list(set(predicted_labels))\n",
    "                prediction_counts = {label: predicted_labels.count(label) for label in unique_predicted_labels}\n",
    "                print(f\"Interval {interval_count}: {interval_time}\")\n",
    "                print(f\"\\tPredictions: {prediction_counts}\")\n",
    "                interval_count += 1\n",
    "\n",
    "        # Measure final resource usage\n",
    "        print(\"\\nFinal Resource Usage:\")\n",
    "        memory_used, cpu_used = measure_resource_usage(initial_memory_usage, initial_cpu_usage)\n",
    "        print(f\"\\tMemory Used: {memory_used:.2f} MB, CPU Used: {cpu_used:.2f}%\\n\")\n",
    "\n",
    "        # Execution time\n",
    "        end_time = time.time()\n",
    "        print(f\"\\nTotal Execution Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path} or {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Log Data to choose\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-17-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-3-1.labeled\"\n",
    "log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-34-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-36-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-60-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/corrected_traffic_dataset.labeled\"\n",
    "\n",
    "# Trained Model to choose\n",
    "model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - DCT-mali.joblib\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - SVM-mali.joblib\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - RF-mali.joblib\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - AdaBoost-mali.joblib\"\n",
    "\n",
    "process_log_in_intervals(log_file_path, model_path, interval_minutes=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import psutil  # Library for monitoring system resources\n",
    "import time\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to generate signatures from a DataFrame\n",
    "def generate_signatures_from_log(df):\n",
    "    signature_counts = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        id_resp_p = row[5]\n",
    "        proto = row[6]\n",
    "        service = row[7]\n",
    "        orig_bytes = row[9]\n",
    "        resp_bytes = row[10]\n",
    "        conn_state = row[11]\n",
    "\n",
    "        # Clean up service and byte values\n",
    "        service = service if service not in ['0', '-', ''] else ''\n",
    "        orig_bytes = orig_bytes if orig_bytes not in ['-', '0'] else '0'\n",
    "        resp_bytes = resp_bytes if resp_bytes not in ['-', '0'] else '0'\n",
    "\n",
    "        # Create the signature\n",
    "        signature = f\"{id_resp_p}{proto}{service}{orig_bytes}{resp_bytes}{conn_state}\"\n",
    "\n",
    "        # Count occurrences of each signature\n",
    "        signature_counts[signature] = signature_counts.get(signature, 0) + 1\n",
    "\n",
    "    output_list = [[signature, frequency] for signature, frequency in signature_counts.items()]\n",
    "    return output_list\n",
    "\n",
    "# Function to monitor and print resource usage\n",
    "def measure_resource_usage():\n",
    "    process = psutil.Process()\n",
    "    current_memory = process.memory_info().rss / (1024 * 1024)  # Memory usage in MB\n",
    "    current_cpu = psutil.cpu_percent(interval=1)  # CPU usage in percentage\n",
    "    return current_memory, current_cpu\n",
    "\n",
    "# Function to dynamically allocate chunk size based on available memory\n",
    "def get_dynamic_chunk_size():\n",
    "    available_memory = psutil.virtual_memory().available / (1024 * 1024)  # in MB\n",
    "    chunk_size = int(min(5000, max(1000, available_memory // 10)))  # scale with available memory\n",
    "    logging.info(f\"Dynamically set chunk size: {chunk_size}\")\n",
    "    return chunk_size\n",
    "\n",
    "# Function to process a batch and log interval predictions\n",
    "def process_batch(interval_buffer, time_buffer, clf, signature_list, label_mapping, batch_id):\n",
    "    for idx, (X_mat, interval_time) in enumerate(zip(interval_buffer, time_buffer)):\n",
    "        # Make predictions for all signatures in the current batch of intervals\n",
    "        predictions = clf.predict(X_mat)\n",
    "\n",
    "        # Convert numerical predictions to corresponding labels\n",
    "        predicted_labels = [label_mapping[pred] for pred in predictions]\n",
    "\n",
    "        # Print the unique predicted labels for the interval\n",
    "        unique_predicted_labels = list(set(predicted_labels))\n",
    "        prediction_counts = {label: predicted_labels.count(label) for label in unique_predicted_labels}\n",
    "        logging.info(f\"Batch {batch_id} Interval {interval_time}: {prediction_counts}\")\n",
    "\n",
    "# Main processing function\n",
    "def process_log_in_intervals(file_path, model_path, interval_minutes=5, intervals_per_batch=5):\n",
    "    try:\n",
    "        # Load the model and signature list\n",
    "        model_data = joblib.load(model_path)\n",
    "        clf = model_data['model']\n",
    "        signature_list = model_data['signature_list']\n",
    "\n",
    "        # Label mapping\n",
    "        label_mapping = {\n",
    "            0: 'Attack',\n",
    "            1: 'C&C',\n",
    "            2: 'C&C-HeartBeat',\n",
    "            3: 'C&C-Torii',\n",
    "            4: 'DDoS',\n",
    "            5: 'Okiru',\n",
    "            6: 'PortScan',\n",
    "            7: 'PortScan-Attack'\n",
    "        }\n",
    "\n",
    "        # Define time intervals in seconds\n",
    "        interval_duration = pd.Timedelta(minutes=interval_minutes)\n",
    "\n",
    "        # Dynamically set chunk size based on system resources\n",
    "        chunk_size = get_dynamic_chunk_size()\n",
    "\n",
    "        # Read the log data in chunks\n",
    "        df_iterator = pd.read_csv(file_path, delim_whitespace=True, header=None, na_filter=False,\n",
    "                                  on_bad_lines='skip', skiprows=8, low_memory=False, chunksize=chunk_size)\n",
    "\n",
    "        # Initialize variables for batch processing\n",
    "        current_time = None\n",
    "        next_time = None\n",
    "        interval_count = 1\n",
    "        interval_buffer = []\n",
    "        time_buffer = []\n",
    "        interval_counter = 0  # Keep track of the number of intervals in a batch\n",
    "\n",
    "        # Initialize resource usage variables\n",
    "        initial_memory_usage, initial_cpu_usage = measure_resource_usage()\n",
    "        max_memory_usage, max_cpu_usage = initial_memory_usage, initial_cpu_usage\n",
    "        start_time = time.time()  # Start time for execution\n",
    "\n",
    "        # Use ThreadPoolExecutor to process batches in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            for chunk in df_iterator:\n",
    "                # Preprocess the chunk\n",
    "                chunk[0] = pd.to_numeric(chunk[0], errors='coerce')\n",
    "                chunk.dropna(subset=[0, 3, 5, 6, 14, 16, 17], inplace=True)\n",
    "                chunk[0] = pd.to_datetime(chunk[0], errors='coerce', unit='s')  # Convert timestamp to datetime\n",
    "                chunk.dropna(subset=[0], inplace=True)\n",
    "\n",
    "                if current_time is None:\n",
    "                    current_time = chunk[0].min()\n",
    "                    next_time = current_time + interval_duration\n",
    "\n",
    "                # Process the chunk in intervals\n",
    "                while current_time < chunk[0].max():\n",
    "                    interval_df = chunk[(chunk[0] >= current_time) & (chunk[0] < next_time)]\n",
    "\n",
    "                    if not interval_df.empty:\n",
    "                        # Generate signatures\n",
    "                        signatures = generate_signatures_from_log(interval_df)\n",
    "\n",
    "                        # Prepare feature matrix\n",
    "                        X_matrix = np.zeros((len(signatures), len(signature_list)), dtype=int)\n",
    "                        for i, (sig, frequency) in enumerate(signatures):\n",
    "                            if sig in signature_list:\n",
    "                                sig_idx = signature_list.index(sig)\n",
    "                                X_matrix[i, sig_idx] = frequency\n",
    "\n",
    "                        # Add to interval buffer\n",
    "                        interval_buffer.append(X_matrix)\n",
    "                        time_buffer.append(current_time)\n",
    "                        interval_counter += 1\n",
    "\n",
    "                    # Process a batch of intervals\n",
    "                    if interval_counter >= intervals_per_batch:\n",
    "                        batch_id = interval_count // intervals_per_batch\n",
    "\n",
    "                        # Log predictions for each interval in the batch\n",
    "                        executor.submit(process_batch, interval_buffer, time_buffer, clf, signature_list, label_mapping, batch_id)\n",
    "\n",
    "                        # Measure resource usage after batch processing\n",
    "                        current_memory_usage, current_cpu_usage = measure_resource_usage()\n",
    "                        max_memory_usage = max(max_memory_usage, current_memory_usage)\n",
    "                        max_cpu_usage = max(max_cpu_usage, current_cpu_usage)\n",
    "\n",
    "                        logging.info(f\"Batch {batch_id} Resource Usage: Memory Used: {current_memory_usage:.2f} MB, CPU Used: {current_cpu_usage:.2f}%\")\n",
    "\n",
    "                        # Clear buffers and reset interval counter after batch processing\n",
    "                        interval_buffer, time_buffer = [], []\n",
    "                        interval_counter = 0\n",
    "                        interval_count += 1\n",
    "\n",
    "                    current_time = next_time\n",
    "                    next_time = current_time + interval_duration\n",
    "\n",
    "            # Process any remaining intervals if they're less than the batch size\n",
    "            if interval_buffer:\n",
    "                batch_id = interval_count // intervals_per_batch\n",
    "                process_batch(interval_buffer, time_buffer, clf, signature_list, label_mapping, batch_id)\n",
    "\n",
    "        # Measure final resource usage\n",
    "        current_memory_usage, current_cpu_usage = measure_resource_usage()\n",
    "        max_memory_usage = max(max_memory_usage, current_memory_usage)\n",
    "        max_cpu_usage = max(max_cpu_usage, current_cpu_usage)\n",
    "\n",
    "        logging.info(f\"Memory Used: {current_memory_usage:.2f} MB, CPU Used: {current_cpu_usage:.2f}%\")\n",
    "\n",
    "        # Execution time\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"Total Execution Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "        # Report maximum resource usage during the testing run\n",
    "        max_memory_used = max_memory_usage - initial_memory_usage\n",
    "        max_cpu_used = max_cpu_usage - initial_cpu_usage\n",
    "        logging.info(\"\\nFinal Resource Usage:\")\n",
    "        logging.info(f\"Max Memory Used During Execution: {max_memory_usage:.2f} MB (Test Memory Usage: {max_memory_used:.2f} MB)\")\n",
    "        logging.info(f\"Max CPU Used During Execution: {max_cpu_usage:.2f}% (Test CPU Usage: {max_cpu_used:.2f}%)\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {file_path} or {model_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Example Log Data to choose\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-17-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-3-1.labeled\"\n",
    "log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-34-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-36-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-60-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/corrected_traffic_dataset.labeled\"\n",
    "\n",
    "# Trained Model to choose\n",
    "model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - DCT-mali.joblib\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - SVM-mali.joblib\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - RF-mali.joblib\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - AdaBoost-mali.joblib\"\n",
    "\n",
    "# Measure initial resource usage\n",
    "logging.info(\"Measuring Initial Resource Usage...\")\n",
    "initial_memory_usage, initial_cpu_usage = measure_resource_usage()\n",
    "logging.info(f\"Initial Memory Usage: {initial_memory_usage:.2f} MB, Initial CPU Usage: {initial_cpu_usage:.2f}%\")\n",
    "\n",
    "process_log_in_intervals(log_file_path, model_path, interval_minutes=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Data to choose\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-17-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-3-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-34-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-36-1.labeled\"\n",
    "log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/Capture-60-1.labeled\"\n",
    "# log_file_path = \"C:/Users/Natty PC/Documents/Party/Project II/Data/corrected_traffic_dataset.labeled\"\n",
    "\n",
    "# Trained Model to choose\n",
    "model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - DCT-mali.joblib\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - SVM-mali.joblib\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - RF-mali.joblib\"\n",
    "# model_path = \"C:/Users/Natty PC/Documents/Party/Project II/Models/Model - AdaBoost-mali.joblib\"\n",
    "\n",
    "# Measure initial resource usage\n",
    "logging.info(\"Measuring Initial Resource Usage...\")\n",
    "initial_memory_usage, initial_cpu_usage = measure_resource_usage()\n",
    "logging.info(f\"Initial Memory Usage: {initial_memory_usage:.2f} MB, Initial CPU Usage: {initial_cpu_usage:.2f}%\")\n",
    "\n",
    "process_log_in_intervals(log_file_path, model_path, interval_minutes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
